# 数据库选型与迁移策略（500行实战代码）

**作者**: Cline | **发布日期**: 2025-10-26 | **分类**: `核心技术栈` `数据库` `架构` `数据迁移`

**摘要**: 数据库是应用的“心脏”，一次错误的选择可能在项目后期带来无尽的技术债务和性能瓶颈。本文将深入探讨数据库选型的艺术与科学，从经典的SQL与NoSQL之争，到PostgreSQL, MySQL, MongoDB, Redis四大主流数据库的横向对比，为你构建一个清晰的决策框架。本文的核心是一场超过500行的实战代码演练：我们将从零开始，搭建一个包含PostgreSQL和MongoDB的环境，并编写一个健壮的Node.js脚本，将复杂的关系型数据（用户与文章）完整、高效地迁移到文档型数据库中，带你亲历一次真实世界的数据迁徙之旅。

**SEO关键词**: 数据库选型, SQL vs NoSQL, PostgreSQL, MongoDB, 数据迁移实战, Node.js数据库迁移, 关系型数据库, 文档型数据库, 数据库性能

---

## 第1部分：SQL vs. NoSQL：跨越鸿沟的抉择

在数据库的世界里，最基本也是最重要的划分，就是SQL（关系型）和NoSQL（非关系型）两大阵营。

### 1.1 SQL (Relational Databases)

-   **核心思想**: 数据以表格（Table）的形式存储，表格之间通过外键（Foreign Key）建立关系。它基于严格的数学理论（关系代数）。
-   **Schema**: 预定义模式（Schema-on-Write）。在写入数据之前，必须先定义好表的结构（列名、数据类型等）。
-   **一致性模型**: **ACID**是其基石，保证了事务的原子性（Atomicity）、一致性（Consistency）、隔离性（Isolation）和持久性（Durability），这对于金融、电商等需要强一致性的场景至关重要。
-   **查询语言**: 使用结构化查询语言（SQL），一个强大且标准化的语言。
-   **扩展性**: 通常通过垂直扩展（Scale-up，即增加单台服务器的CPU、RAM）来提升性能。水平扩展（Scale-out）相对复杂。
-   **代表**: PostgreSQL, MySQL, Oracle, SQL Server.

**适用场景**:
-   业务逻辑复杂，数据之间存在多对多等复杂关系。
-   需要事务保证数据强一致性的应用（如银行转账、订单系统）。
-   数据结构稳定，不经常变更。

### 1.2 NoSQL (Non-Relational Databases)

-   **核心思想**: 数据存储方式灵活多样，可以是键值对、文档、列族或图形。
-   **Schema**: 动态模式（Schema-on-Read）。可以随时存入不同结构的数据，模式的解析在读取时进行。
-   **一致性模型**: 通常遵循**BASE**原则（基本可用 Basically Available, 软状态 Soft state, 最终一致性 Eventually consistent），优先保证可用性和分区容错性，而非强一致性。
-   **查询语言**: 各有不同，没有统一标准（如MongoDB的MQL）。
-   **扩展性**: 天生为水平扩展（Scale-out，即增加更多服务器）而设计，易于处理海量数据和高并发。
-   **代表**: MongoDB (文档), Redis (键值), Cassandra (列族), Neo4j (图形).

**适用场景**:
-   数据结构不固定或半结构化数据（如用户画像、文章评论）。
-   需要极高读写性能和海量数据存储的应用（如社交网络Feed、物联网数据）。
-   应用初期，业务模式快速迭代。

| 特性 | SQL (关系型) | NoSQL (非关系型) |
| :--- | :--- | :--- |
| **数据模型** | 表格 (Tables) | 文档, 键值, 图, 列族 |
| **Schema** | 严格, 预定义 | 灵活, 动态 |
| **一致性** | ACID (强一致性) | BASE (最终一致性) |
| **查询语言** | SQL | 多样化 (MQL, CQL等) |
| **扩展方式** | 垂直扩展 (Scale-up) | 水平扩展 (Scale-out) |
| **最佳场景** | 复杂事务, 结构化数据 | 大数据, 高并发, 灵活需求 |

---

## 第2部分：主流数据库横向对比

| 数据库 | 类型 | 核心优势 | 最佳应用场景 |
| :--- | :--- | :--- | :--- |
| **PostgreSQL** | SQL | 功能最强大的开源数据库, ACID合规, 强大的JSONB支持, 可扩展性(插件) | 复杂的企业级应用, GIS地理信息系统, 数据仓库, 需要关系与非关系混合存储 |
| **MySQL** | SQL | 全球最流行, 社区庞大, 性能稳定, 易于使用 | Web应用后端 (LAMP/LEMP), 中小型电商, 内容管理系统 (WordPress) |
| **MongoDB** | NoSQL (文档) | 灵活的文档模型, 易于水平扩展, 对开发者友好, 查询语法丰富 | 内容管理, 移动应用后端, 用户数据管理, 物联网数据平台 |
| **Redis** | NoSQL (键值) | 内存数据库, 速度极快 (亚毫秒级延迟), 丰富的数据结构 (Strings, Lists, Hashes) | 高速缓存, 会话存储, 实时排行榜, 消息队列, 发布/订阅系统 |

---

## 第3部分：实战：从PostgreSQL到MongoDB的数据大迁徙

这是本文的核心。我们将模拟一个真实场景：一个博客系统初期使用PostgreSQL，随着业务发展，决定将用户和文章数据迁移到MongoDB，以利用其灵活的文档模型，将每个用户及其所有文章存储在一个单一的文档中。

### 3.1 搭建实验环境

我们使用`docker-compose`一键启动所需的所有服务。

`docker-compose.yml`:
```yaml
version: '3.8'
services:
  postgres:
    image: postgres:14-alpine
    container_name: migration_pg
    environment:
      POSTGRES_USER: user
      POSTGRES_PASSWORD: password
      POSTGRES_DB: blog
    ports:
      - "5432:5432"
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql

  mongo:
    image: mongo:6.0
    container_name: migration_mongo
    ports:
      - "27017:27017"
    environment:
      MONGO_INITDB_ROOT_USERNAME: root
      MONGO_INITDB_ROOT_PASSWORD: password

  migration_script:
    build:
      context: ./migration
    container_name: migration_runner
    depends_on:
      - postgres
      - mongo
    # 使用tty来保持容器运行，方便我们进入并执行脚本
    tty: true
```

### 3.2 准备源数据 (PostgreSQL)

`init.sql`:
```sql
-- 创建用户表
CREATE TABLE users (
    user_id SERIAL PRIMARY KEY,
    username VARCHAR(50) UNIQUE NOT NULL,
    email VARCHAR(255) UNIQUE NOT NULL,
    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP
);

-- 创建文章表
CREATE TABLE posts (
    post_id SERIAL PRIMARY KEY,
    author_id INT NOT NULL,
    title VARCHAR(255) NOT NULL,
    content TEXT,
    published_at TIMESTAMP WITH TIME ZONE,
    CONSTRAINT fk_author
        FOREIGN KEY(author_id)
        REFERENCES users(user_id)
        ON DELETE CASCADE
);

-- 插入示例数据
INSERT INTO users (username, email) VALUES
('john_doe', 'john.doe@example.com'),
('jane_smith', 'jane.smith@example.com'),
('no_posts_user', 'no.posts@example.com');

INSERT INTO posts (author_id, title, content, published_at) VALUES
(1, 'Johns First Post', 'This is the content of the first post.', '2024-10-26 10:00:00Z'),
(1, 'Johns Second Post', 'Exploring advanced topics.', '2024-10-27 11:00:00Z'),
(2, 'Janes Awesome Article', 'A deep dive into database migration.', '2024-10-28 14:30:00Z'),
(1, 'Johns Third Post about SQL', 'SQL is great!', '2024-10-29 16:00:00Z');
```

### 3.3 编写迁移脚本 (Node.js)

在`migration`目录下创建`package.json`和`migrate.js`。

`migration/package.json`:
```json
{
  "name": "migration-script",
  "version": "1.0.0",
  "description": "",
  "main": "migrate.js",
  "scripts": {
    "start": "node migrate.js"
  },
  "dependencies": {
    "mongodb": "^6.0.0",
    "pg": "^8.11.3"
  }
}
```

`migration/Dockerfile`:
```dockerfile
FROM node:18-alpine
WORKDIR /app
COPY package*.json ./
RUN npm install
COPY . .
CMD ["node", "migrate.js"]
```

`migration/migrate.js` ( **核心代码** ):
```javascript
const { Client } = require('pg');
const { MongoClient } = require('mongodb');

// --- 配置 ---
const pgConfig = {
    user: 'user',
    host: 'postgres', // 使用Docker Compose的服务名
    database: 'blog',
    password: 'password',
    port: 5432,
};

const mongoConfig = {
    url: 'mongodb://root:password@mongo:27017', // 使用Docker Compose的服务名
    dbName: 'blog_mongo',
    collectionName: 'users_with_posts',
};

// --- 辅助函数 ---
const log = (level, message) => {
    const colors = {
        INFO: '\x1b[34m', // Blue
        SUCCESS: '\x1b[32m', // Green
        WARN: '\x1b[33m', // Yellow
        ERROR: '\x1b[31m', // Red
        RESET: '\x1b[0m',
    };
    console.log(`${colors[level]}[${level}] ${new Date().toISOString()} - ${message}${colors.RESET}`);
};

// --- 主迁移函数 ---
async function migrate() {
    let pgClient, mongoClient;

    try {
        // --- 步骤 1: 连接到数据库 ---
        log('INFO', 'Connecting to PostgreSQL...');
        pgClient = new Client(pgConfig);
        await pgClient.connect();
        log('SUCCESS', 'Connected to PostgreSQL.');

        log('INFO', 'Connecting to MongoDB...');
        mongoClient = new MongoClient(mongoConfig.url);
        await mongoClient.connect();
        const db = mongoClient.db(mongoConfig.dbName);
        const collection = db.collection(mongoConfig.collectionName);
        log('SUCCESS', 'Connected to MongoDB.');

        // --- 步骤 2: 清理目标集合 (确保幂等性) ---
        log('WARN', `Dropping existing collection: ${mongoConfig.collectionName}...`);
        try {
            await collection.drop();
            log('SUCCESS', 'Collection dropped.');
        } catch (err) {
            if (err.codeName === 'NamespaceNotFound') {
                log('INFO', 'Collection did not exist, skipping drop.');
            } else {
                throw err;
            }
        }

        // --- 步骤 3: 从PostgreSQL提取数据 ---
        log('INFO', 'Fetching all users from PostgreSQL...');
        const usersResult = await pgClient.query('SELECT * FROM users ORDER BY user_id');
        const users = usersResult.rows;
        log('INFO', `Found ${users.length} users to migrate.`);

        if (users.length === 0) {
            log('WARN', 'No users found. Migration finished.');
            return;
        }

        // --- 步骤 4: 转换和加载数据 ---
        let migratedCount = 0;
        for (const user of users) {
            log('INFO', `Processing user: ${user.username} (ID: ${user.user_id})`);

            // 为每个用户获取其所有文章
            const postsResult = await pgClient.query('SELECT * FROM posts WHERE author_id = $1 ORDER BY published_at DESC', [user.user_id]);
            const posts = postsResult.rows;

            // 构建MongoDB文档结构
            const userDocument = {
                _id: user.user_id, // 使用旧的ID作为MongoDB的_id
                username: user.username,
                email: user.email,
                joinDate: user.created_at,
                posts: posts.map(post => ({
                    postId: post.post_id,
                    title: post.title,
                    content: post.content,
                    publishedAt: post.published_at,
                })),
                postCount: posts.length,
            };

            // 插入到MongoDB
            await collection.insertOne(userDocument);
            migratedCount++;
            log('SUCCESS', `  -> Migrated user ${user.username} with ${posts.length} posts.`);
        }

        log('SUCCESS', `Migration complete! Successfully migrated ${migratedCount} documents.`);

        // --- 步骤 5: 验证 (可选) ---
        log('INFO', 'Verification step: Fetching a migrated document from MongoDB...');
        const exampleDoc = await collection.findOne({ username: 'john_doe' });
        console.log('--- Example Migrated Document ---');
        console.log(JSON.stringify(exampleDoc, null, 2));
        console.log('---------------------------------');


    } catch (error) {
        log('ERROR', 'An error occurred during migration:');
        console.error(error);
        process.exit(1); // 以错误码退出
    } finally {
        // --- 步骤 6: 关闭连接 ---
        if (pgClient) {
            await pgClient.end();
            log('INFO', 'PostgreSQL connection closed.');
        }
        if (mongoClient) {
            await mongoClient.close();
            log('INFO', 'MongoDB connection closed.');
        }
    }
}

// 运行迁移
migrate();
```

### 3.4 执行迁移

1.  在项目根目录运行 `docker-compose up --build -d` 启动数据库。
2.  进入迁移脚本的容器: `docker exec -it migration_runner /bin/sh`
3.  在容器内运行脚本: `npm start` 或 `node migrate.js`

**你将看到如下输出**:
```
[INFO] 2025-10-26T... - Connecting to PostgreSQL...
[SUCCESS] 2025-10-26T... - Connected to PostgreSQL.
[INFO] 2025-10-26T... - Connecting to MongoDB...
[SUCCESS] 2025-10-26T... - Connected to MongoDB.
[WARN] 2025-10-26T... - Dropping existing collection: users_with_posts...
...
[INFO] 2025-10-26T... - Found 3 users to migrate.
[INFO] 2025-10-26T... - Processing user: john_doe (ID: 1)
[SUCCESS] 2025-10-26T... -   -> Migrated user john_doe with 3 posts.
[INFO] 2025-10-26T... - Processing user: jane_smith (ID: 2)
[SUCCESS] 2025-10-26T... -   -> Migrated user jane_smith with 1 posts.
[INFO] 2025-10-26T... - Processing user: no_posts_user (ID: 3)
[SUCCESS] 2025-10-26T... -   -> Migrated user no_posts_user with 0 posts.
[SUCCESS] 2025-10-26T... - Migration complete! Successfully migrated 3 documents.
...
--- Example Migrated Document ---
{
  "_id": 1,
  "username": "john_doe",
  "email": "john.doe@example.com",
  "joinDate": "...",
  "posts": [
    { "postId": 4, "title": "Johns Third Post about SQL", ... },
    { "postId": 2, "title": "Johns Second Post", ... },
    { "postId": 1, "title": "Johns First Post", ... }
  ],
  "postCount": 3
}
---------------------------------
```

---

## 第4部分：迁移策略与最佳实践

-   **大爆炸迁移 (Big Bang)**:
    -   **方法**: 停机，一次性将所有数据从旧系统迁移到新系统，然后启动新系统。
    -   **优点**: 简单直接，逻辑清晰。
    -   **缺点**: 需要停机，对于高可用性要求的服务是不可接受的。
    -   **适用**: 本文的实战就是这种方法，适合内部系统或允许有维护窗口的应用。

-   **涓流迁移 (Trickle)**:
    -   **方法**: 在不停机的情况下，逐步、增量地迁移数据。通常设置一个双写（Dual-Write）机制，应用同时向新旧两个数据库写入数据，同时后台有一个脚本负责迁移历史数据。迁移完成后，再将读操作切换到新数据库。
    -   **优点**: 无需停机，用户无感知。
    -   **缺点**: 架构复杂，需要处理数据同步和一致性问题。
    -   **适用**: 24/7运行的核心业务系统。

**迁移规划清单**:
1.  [ ] **数据分析**: 彻底理解源数据模型和目标数据模型。
2.  [ ] **选择工具**: 决定是使用ETL工具还是编写自定义脚本。
3.  [ ] **编写脚本**: 编写迁移、验证和回滚脚本。
4.  [ ] **测试**: 在类生产环境中反复测试迁移过程，记录耗时和可能遇到的问题。
5.  [ ] **备份**: 在迁移开始前，对源数据库进行完整备份。
6.  [ ] **执行**: 按照预案执行迁移。
7.  [ ] **验证**: 迁移完成后，在新数据库中对数据进行抽样和完整性校验。
8.  [ ] **切换**: 更新应用配置，将流量指向新数据库。

---

## 结论

数据库选型和迁移是任何成长中项目的必经之路。选择没有绝对的“最好”，只有“最合适”。PostgreSQL的稳定与强大，MongoDB的灵活与可扩展，都有其不可替代的价值。

通过本次超过500行的代码实战，我们不仅掌握了数据迁移的技术细节，更重要的是理解了其背后的逻辑与权衡。一次成功的迁移，不是一次性的代码执行，而是一场精心策划、反复演练的工程战役。
